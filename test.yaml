game:
    test_model: False # True in no training happens
    checkpoint_name: "sac_20201216_17-25-51" # Date and time of the experiments. Used loading the model created that date (if asked by the user) 
    load_checkpoint: False # True if loading stored model
    second_human: False # False if playing with RL agent
    agent_only: False # True if playing only the RL agent (no human-in-the-loop)
    verbose: True # Used for logging
    save: True # Save models and logs
    discrete: False # Discrete or continuous human input
    
    # position of the goal on the board
    goal: "left_down" # "left_down" "left_up" "right_down"

SAC:
  # SAC parameters
  # NOTE: Only discrete SAC is compatible with the game so far
  discrete: True
  layer1_size: 32 # Number of variables in hidden layer
  layer2_size: 32 # Number of variables in hidden layer
  batch_size: 256
  gamma: 0.99  # discount factor
  tau: 0.005
  alpha: 0.0003
  beta: 0.0003
  target_entropy_ratio: 0.4

  # Type of reward function
  # Currently three reward functions are implemented
  # The implementation and the description are in the game/rewards.py file
  # Choose on the the following: Shafti, Distance, Timeout
  reward_function: Distance

Experiment:
  online_updates: False # True if a signle gradient update happens after every state transition
  test_interval: 10 # Test the current model after `test_interval` episodes

  # offline gradient updates allocation
  # Normal: allocates evenly the total number of updates through each session
  # descending: allocation of total updates using geometric progression with ratio 1/2
  scheduling: normal # descending normal

  # Max episodes mode
  max_episodes: 70  # Total episodes per game
  max_duration: 0.5  # Max duration of an episode (in seconds). An episode ends if the ball hits the target or if we reach the time limit
  buffer_max_size: 1000000
  action_duration: 0.2 # Time duration between consecutive RL agent actions
  start_training_on_episode: 1 # Will not train the agent before this trial
  stop_random_agent: 10 # Stop using random agent on this trial and start using SAC
  learn_every_n_episodes: 1 # Perform offline gradient updates after every `learn_every_n_episodes` episodes
  total_update_cycles: 28000 # Total number of offline gradient updates throughout the whole experiment
  reward_scale: 2
  log_interval: 10  # Print avg reward in the interval

