Game:
  controlled_variable: "acc" # variable to control. vel means that the actions are the commanded velocity. Alternatively, the actions can be the commanded acceleration
  participant_name: "thanasis"
  test_model: False # True if no training happens
  checkpoint_name: "sac_20201216_17-25-51" # Date and time of the experiments. Used loading the model created that date (if asked by the user) 
  load_checkpoint: False # True if loading stored model
  save: True # Save models and logs

SAC:
  # SAC parameters
  layer1_size: 32 # Number of variables in hidden layer
  layer2_size: 32 # Number of variables in hidden layer
  batch_size: 256
  gamma: 0.99  # discount factor
  tau: 0.005
  alpha: 0.0003
  beta: 0.0003
  target_entropy_ratio: 0.4

Experiment:
  chkpt_dir: "rl_models"
  online_updates: False # True if a signle gradient update happens after every state transition
  test_interval: 10 # Test the current model after `test_interval` episodes

  # Max episodes mode
  max_episodes: 70  # Total episodes per game
  max_duration: 20  # Max duration of an episode (in seconds). An episode ends if the ball hits the target or if we reach the time limit
  buffer_max_size: 1000000
  action_duration: 0.2 # Time duration between consecutive RL agent actions
  start_training_on_episode: 10 # Will not train the agent before this trial
  stop_random_agent: 10 # Stop using random agent on this trial and start using SAC
  learn_every_n_episodes: 10 # Perform offline gradient updates after every `learn_every_n_episodes` episodes
  total_update_cycles: 28000 # Total number of offline gradient updates throughout the whole experiment
  reward_scale: 2
  log_interval: 10  # Print avg reward in the interval

  test:
    max_episodes: 10
    max_duration: 40

